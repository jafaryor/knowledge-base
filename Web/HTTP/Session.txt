In client-server protocols, like HTTP, sessions consist of three phases:
    * The client establishes a TCP connection (or the appropriate connection
        if the transport layer is not TCP).
        In client-server protocols, it is the client which establishes the connection.
        Opening a connection in HTTP means initiating a connection in the underlying
        transport layer, usually this is TCP.
    * The client sends its request, and waits for the answer.
    * The server processes the request, sending back its answer, providing
        a status code and appropriate data.

As of HTTP/1.1, the connection is no longer closed after completing the third phase,
    and the client is now granted a further request: this means the second and third
    phases can now be performed any number of times.

With TCP the default port, for an HTTP server on a computer, is port 80. Other
    ports can also be used, like 8000 or 8080. The URL of a page to fetch contains
    both the domain name, and the port number, though the latter can be omitted
    if it is 80.


Connection management in HTTP/1.x
    HTTP mostly relies on TCP for its transport protocol, providing a connection between
        the client and the server.
    It's important point to note that connection management in HTTP applies to the
        connection between two consecutive nodes, which is hop-by-hop and not end-to-end.

    Short-lived connections
        The original model of HTTP, and the default one in HTTP/1.0, is short-lived
        connections. Each HTTP request is completed on its own connection; this means a
        TCP handshake happens before each HTTP request, and these are serialized.
        
        This model is the default model used in HTTP/1.0 (if there is no Connection header,
        or if its value is set to close). In HTTP/1.1, this model is only used when the
        Connection header is sent with a value of close.

    Persistent connections
        Short-lived connections have two major hitches: the time taken to establish a new
        connection is significant, and performance of the underlying TCP connection gets
        better only when this connection has been in use for some time (warm connection).
        To ease these problems, the concept of a persistent connection has been designed,
        even prior to HTTP/1.1. Alternatively this may be called a keep-alive connection.

        A persistent connection is a one which remains open for a period, and can be reused
        for several requests, saving the the need for a new TCP handshake, and utilizing
        TCP's performance enhancing capabilities. This connection will not stay open forever:
        idle connections are closed after some time (a server may use the Keep-Alive header
        to specify a minimum time the connection should be kept open).

        Drawbacks; even when idling they consume server resources, and under heavy load,
        DoS attacks can be conducted. In such cases, using non-persistent connections,
        which are closed as soon as they are idle, can provide better performance.

        HTTP/1.0 connections are not persistent by default. Setting Connection to anything
        other than close, usually retry-after, will make them persistent.

        In HTTP/1.1, persistence is the default, and the header is no longer needed (but it
        is often added as a defensive measure against cases requiring a fallback to HTTP/1.0).

    HTTP pipelining
        HTTP pipelining is not activated by default in modern browsers:
            * Buggy proxies are still common and these lead to strange and erratic behaviors
                that Web developers cannot foresee and diagnose easily.
            * Pipelining is complex to implement correctly: the size of the resource being
                transferred, the effective RTT that will be used, as well as the effective
                bandwidth, have a direct incidence on the improvement provided by the pipeline.
                Without knowing these, important messages may be delayed behind unimportant ones.
                The notion of important even evolves during page layout! HTTP pipelining therefore
                brings a marginal improvement in most cases only.
            * Pipelining is subject to the HOL problem.
        For these reasons, pipelining has been superseded by a better algorithm, multiplexing,
        that is used by HTTP/2.

        By default, HTTP requests are issued sequentially. The next request is only issued once the
        response to the current request has been received. As they are affected by network latencies
        and bandwidth limitations, this can result in significant delay before the next request is
        seen by the server.

        Pipelining is the process to send successive requests, over the same persistent connection,
        without waiting for the answer. This avoids latency of the connection. Theoretically,
        performance could also be improved if two HTTP requests were to be packed into the same
        TCP message. The typical MSS (Maximum Segment Size), is big enough to contain several simple
        requests, although the demand in size of HTTP requests continues to grow.

        Not all types of HTTP requests can be pipelined: only idempotent methods, that is GET, HEAD,
        PUT and DELETE, can be replayed safely: should a failure happen, the pipeline content can
        simply be repeated.

    Domain sharding
        As an HTTP/1.x connection is serializing requests, even without any ordering, it can't be
        optimal without large enough available bandwidth. As a solution, browsers open several
        connections to each domain, sending parallel requests. Default was once 2 to 3 connections,
        but this has now increased to a more common use of 6 parallel connections. There is a risk of
        triggering DoS protection on the server side if attempting more than this number.

        If  the server wishes a faster Web site or application response, it is possible for the server
        to force the opening of more connections. For example, Instead of having all resources on the
        same domain, say www.example.com, it could split over several domains, www1.example.com,
        www2.example.com, www3.example.com. Each of these domains resolve to the same server, and
        the Web browser will open 6 connections to each (in our example, boosting the connections
        to 18). This technique is called domain sharding.

        ! Unless you have a very specific immediate need, don't use this deprecated technique; switch
        to HTTP/2 instead. In HTTP/2, domain sharding is no more useful: the HTTP/2 connection is able
        to handle parallel unprioritized requests very well. Domain sharding is even detrimental to
        performance. Most HTTP/2 implementation use a technique called connection coalescing to revert
        eventual domain sharding.