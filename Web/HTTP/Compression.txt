Compression is an important way to increase the performance of a Web site. For some
    documents, size reduction of up to 70% lowers the bandwidth capacity needs.

In practice, web developers don't need to implement compression mechanisms, both
    browsers and servers have it implemented already, but they have to be sure
    that the server is configured adequately. Compression happens at three
    different levels:
        * first some file formats are compressed with specific optimized methods,
        * then general encryption can happen at the HTTP level (the resource is
            transmitted compressed from end to end),
        * and finally compression can be defined at the connection level, between
            two nodes of an HTTP connection.

File format compression
    Each data type has some redundancy, that is wasted space, in it. If text can
    typically have as much as 60% redundancy, this rate can be much higher for
    some other media like audio and video. Unlike text, these other media types
    are taking a lot of space to store and the need to regain this wasted space
    appeared very early. Engineers designed optimized compression algorithm used
    by file formats designed for specific purpose. Compression algorithms used
    for files can be grouped in two wide categories:
        * Loss-less compression, where the compression-uncompression cycle doesn't
            alter the data that is recovered. It matches (byte to byte) with the
            original.
            For images, gif or png are using loss-less compression.
        * Lossy compression, where the cycle alters the original data, in an
            imperceptible way for the user.
            Video formats on the Web are lossy and for images, jpeg is.
    Some formats can be used for both loss-less or lossy compression, like webp,
    and usually lossy algorithm can be configured to compress more or less, which
    then of course leads to less or more quality.
    ! Lossy compression algorithms are usually more efficient that loss-less ones.

End-to-end compression
    For compression, end-to-end compression is where the largest performance
    improvements of Web sites reside. End-to-end compression refers to a compression
    of the body of a message that is done by the server and will last unchanged
    until it reaches the client. Whatever the intermediate nodes are, they
    leave the body untouched.

    Compression algorithms: gzip, the most common one, and br the new challenger.

    The browser sends an Accept-Encoding header with the algorithm it supports and
    its order of precedence, the server picks one, uses it to compress the body of
    the response and uses the Content-Encoding header to tell the browser the
    algorithm it has chosen. As content negotiation has been used to choose a
    representation based on its encoding, the server must send a Vary header
    containing at least Accept-Encoding alongside this header in the response;
    that way, caches will be able to cache the different representations of the resource.

    As compression brings significant performance improvements, it is recommended
    to activate it for all files, but already compressed ones like images,
    audio files and videos.

Hop-by-hop compression
    Hop-by-hop compression, though similar to end-to-end compression, differs by one
    fundamental element: the compression doesn't happen on the resource in the server,
    creating a specific representation that is then transmitted, but on the body of
    the message between any two nodes on the path between the client and the server.
    Connections between successive intermediate nodes may apply a different compression.

    To do this, HTTP uses a mechanism similar to the content negotiation for end-to-end
    compression: the node transmitting the request advertizes its will using the TE
    header and the other node chooses the adequate method, applies it, and indicates
    its choice with the Transfer-Encoding header.